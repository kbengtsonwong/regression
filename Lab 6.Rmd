---
title: "Lab 6"
author: "Kevin Wong"
date: "December 5, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This  dataset found in "hitters.csv" records  the salary  of n = 263  Major League  Baseball  players  during  the  1987  season  as  well  as p = 19  statistics  associated with the performance of each player during the previous season. Specifically,the dataset contains observations from the following variables:  


•AtBat: Number oftimes at bat in 1986  
•Hits: Number of hits in 1986  
•HmRun: Number of home runs in 1986  
•Runs:Number of runs in 1986  
•RBI: Number of runs batted in in 1986  
•Walks: Number of walks in 1986  
•Years: Number of years in the major leagues  
•CAtBat: Number of times at bat during his career
•CHits: Number of hits during his career  
•CHmRun: Number of home runs during his career  
•CRuns: Number of runs during his career  
•CRBI: Number of runs batted in during his career  
•CWalks: Number of walks during his career  
•League: A categorical variablewith levels A(for American) and N(for National) indicating the player’s league at the end of 1986  
•Division: A  factor  with  levels E(for  East) and W(for  West) indicating  the player’s division at the end of 1986  
•PutOuts: Number of put outs in 1986  
•Assists: Number of assistsin 1986  
•Errors: Number of errors in 1986  
•Salary: 1987 annual salary on opening day in thousands of dollars  
•NewLeague: A  factor  with  levels Aand Nindicating  the  player’s  league  at  the beginning of 1987  
  
### Importing Data ###

```{r}
setwd("/Users/kevinwong2014/documents/usf/fall2018/linearregression")
df <- read.csv(file = "hitters.csv", header = T)
n <- dim(df)[1]
```

Let's examine the first five rows of the dataset.
```{r}
df[1:5,]
```

Setting seed for reproducibility. 
```{r}
set.seed(1)
```

### Splitting Data into Train and Test Sets ###  
Generating a random set of training indices. 
```{r}
train_idx <- sample(1:n, size = 210)
length(train_idx)
```

Subtract the training indices out of the (1:n) to get the test indices.
```{r}
test_idx <- (1:n)[-train_idx]
length(test_idx)
```

```{r}
library(glmnet)
```


### Fiting Lasso and Ridge Regression Models ###  
First, we generate a grid for lambda values to iterate over to search for the optimal lambda parameter.
```{r}
grid <- 10^seq(-3, 10, length = 1000)
```

```{r}
X <- model.matrix(Salary ~ ., df)[,-1]
y <- df$Salary
```

Iterating over the lambdas to generate 1000 ridge regression models and plotting lambda vs coefficients.  Each colored line represents a feature's corresponding beta value as lambda increases and they are shrunk closer to 0.  
```{r}
ridge.mod <- glmnet(X[train_idx,], y[train_idx], alpha=0, lambda=grid)
plot(ridge.mod, xvar = "lambda", label = TRUE)
```

Iterating over the lambdas to generate 1000 lasso regression models and plotting lambda vs coefficients.
```{r}
lasso.mod <- glmnet(X[train_idx,], y[train_idx], alpha=1, lambda=grid)
plot(lasso.mod, xvar = "lambda", label = TRUE)
```


### Finding the Best Ridge Model ### 

Use default k=10 fold cross validation on the training data.  
```{r}
cv.out <- cv.glmnet(X[train_idx,], y[train_idx], alpha=0)
plot(cv.out)
```

The best lambda value:
```{r}
bestlam.r <- cv.out$lambda.min
bestlam.r
```

```{r}
log(bestlam.r)
```


The corresponding beta coefficients: 
```{r}
predict(ridge.mod, s=bestlam.r, type = "coefficients") 
```

### Finding the Best Lasso Model ###

Use default k=10 fold cross validation on the training data.
```{r}
cv.out <- cv.glmnet(X[train_idx,], y[train_idx], alpha=1)
plot(cv.out)
```

The best lambda value:
```{r}
bestlam.l <- cv.out$lambda.min
bestlam.l
```

```{r}
log(bestlam.l)
```

The corresponding beta coefficients: 
```{r}
predict(lasso.mod, s=bestlam.l, type = "coefficients")[1:12,]
```

### Compare and Contrast the Models ### 
The comparison between the optimal lasso and ridge models begins with an examination of the lambda term.  Using a lasso penalization term, the lambda ends up being a small value close to 2 whereas the lambda associated with a ridge penalty term is an order of magnitude larger.  Moreover, the variable inclusion/exlclusion is different as lasso regularization allows for some beta terms to be shrunk completely to 0 and thus eliminated from the model. In contrast ridge regularization shrinks these same terms (that were eliminated in lasso) to instead values close to and approaching 0. The last difference between the two models is that the betas even for significant terms still vary by a considerable margin.  



### Lasso, Ridge, and Stepwise Model Performance on Hold-Out Data ### 
```{r}
library(boot)

# Calculating predictive RMSE for ridge regression
ridge.pred <- predict(ridge.mod, s=bestlam.r, newx=X[test_idx,])
rmse_ridge <- sqrt(mean((ridge.pred - y[test_idx])^2))

# Calculating predictive RMSE for lasso regression
lasso.pred <- predict(lasso.mod, s=bestlam.l, newx=X[test_idx,])
rmse_lasso <- sqrt(mean((lasso.pred - y[test_idx])^2))

# Calculating predictive RMSE from the Stepwise Model derived in Lab 5 
glm_stepwise <- glm(Salary ~ CRBI + Hits + PutOuts + Division + AtBat +  Walks + CWalks + CRuns + CAtBat + Assists, data = df)
stepwise.pred <- predict(object = glm_stepwise, newdata = df[test_idx,])
rmse_stepwise <- sqrt(mean((stepwise.pred - y[test_idx])^2))
```

```{r}
results <- data.frame(r = ridge.pred, l = lasso.pred, stepwise = stepwise.pred, actual = y[sort(test_idx)])
colnames(results) <- c("ridge","lasso","stepwise","actual")
results
```

```{r}
rmse_lasso
rmse_ridge
rmse_stepwise
```

Based on these root mean squared error values, the stepwise model is the best performing on unseen data.  This is expected, as the stepwise selection process involves consideration of many distinct models and incorporates a comparison of an accuracy metric within the selection process itself.  Instead, ridge and lasso regularization are approximation techniques that shrink betas from the full model down to optimal values given a penalty term.  Consequently, they sacrifice marginal gains in accuracy for speed.





